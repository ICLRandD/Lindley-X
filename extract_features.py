"""
This script extracts features for testing the Lindley-X reportability model.
The script takes a path to a directory of plain text files of judgments and processes the 
judgment text to extract a range features that can be used to predict whether an 
the judgment is reportable or not. 

$ python3 extract_features.py path/to/text/files path/to/output/output.csv 10
"""

import pandas as pd
from pathlib import Path
import spacy
from spacy.tokens import Doc
from typing import List, Tuple
from wasabi import Printer
import plac
import glob

msg = Printer()
nlp = spacy.load("en_blackstone_proto")

merge_ents = nlp.create_pipe("merge_entities")
nlp.add_pipe(merge_ents, last=True)

FEATURES = []


def predicted_stats(document: str) -> Tuple:
    """
    Extract statistics from the documents. Takes a string 
    and returns a tuple of stats.
    """
    doc = make_doc(document)
    token_count = count_tokens(doc)
    ent_count = count_entities(doc)
    cases_cited_count = total_cases_cited(doc)
    sentence_count = total_sentences(doc)
    cat_counts = total_cat_types(document)
    return (
        token_count,
        ent_count,
        cases_cited_count,
        sentence_count,
        cat_counts[0],
        cat_counts[1],
        cat_counts[2],
        cat_counts[3],
        cat_counts[4],
    )


def make_doc(document: str) -> Doc:
    """
    Creat the spaCy Doc object from the source file.
    Takes a str, returns a Doc.
    """
    # prevent spaCy error: [E088] where string length exceeds 1000000 chars
    if len(document) > 1000000:
        document = document[:999998]
    doc = nlp(document)
    return doc


def count_tokens(doc: Doc) -> int:
    """Count the tokens in the document"""
    total_tokens = len([token for token in doc])
    return total_tokens


def count_entities(doc: Doc) -> int:
    """Count the entities in the document"""
    total_ents = len([ent for ent in doc.ents])
    return total_ents


def entity_to_tokens(total_tokens: int, total_ents: int) -> float:
    """Return the percentage of entities to tokens"""
    return total_ents / total_tokens


def total_cases_cited(doc: Doc) -> int:
    """Total cases cited"""
    total_ents = len([ent for ent in doc.ents if ent.label_ == "CITATION"])
    return total_ents


def total_sentences(doc: Doc) -> int:
    """Total sentences"""
    sentences = len([sent for sent in doc.sents])
    return sentences


def get_top_cat(doc: Doc) -> str:
    """
    Function to identify the highest scoring category
    prediction generated by the text categoriser. 
    """
    cats = doc.cats
    max_score = max(cats.values())
    max_cats = [k for k, v in cats.items() if v == max_score]
    max_cat = max_cats[0]
    return max_cat


def total_cat_types(document: str) -> Tuple:
    """Returns five-value tuple of sentence-level categorisations"""

    doc = make_doc(document)
    sentences = [sent.text for sent in doc.sents]

    uncat = 0
    axiom = 0
    conclusion = 0
    issue = 0
    legal_test = 0

    for doc in nlp.pipe(sentences, batch_size=50):
        top_cat = get_top_cat(doc)
        if top_cat == "AXIOM":
            axiom += 1
        elif top_cat == "CONCLUSION":
            conclusion += 1
        elif top_cat == "ISSUE":
            issue += 1
        elif top_cat == "LEGAL_TEST":
            legal_test += 1
        else:
            uncat += 1

    total_cats = uncat + axiom + conclusion + issue + legal_test
    assert total_cats == len(sentences)

    return (axiom, conclusion, issue, legal_test, uncat)


def check_num_files(input_dir: Path, observations: int) -> None:
    """
    Check whether the number of observations requested exceeds the 
    number of files in the input_dir
    """
    num_files = len(list(input_dir.glob("*")))
    if num_files < observations:
        msg.warn(
            f"""You have asked to extract {observations} samples, but your input directory only has {num_files} file(s). /
            Only {num_files} samples(s) will be extracted."""
        )


@plac.annotations(
    input_dir=("Path to directory with text files", "positional", None, Path),
    output_file=("Output CSV file", "positional", None, Path),
    observations=("Number of samples to extract", "positional", None, int),
)
def main(input_dir=None, output_file=None, observations=None):
    iter = 0

    observations = observations - 1
    directory = Path(input_dir)
    msg.info(f"Iterating over files and extracting sample(s).")
    for filename in directory.iterdir():
        check_num_files(input_dir, observations)
        source_dir = str(input_dir)
        f = open(filename)
        document = f.read()
        document = document.replace("\n", " ")
        casename = filename.stem

        # get predicted features
        predicted_features = predicted_stats(document)
        # calculate percentage of entities to tokens
        if predicted_features[0] != 0 and predicted_features[1] != 0:
            ents_to_tokens = predicted_features[0] / predicted_features[1]
        else:
            break
        # get total special category sentences
        total_special = (
            predicted_features[4]
            + predicted_features[5]
            + predicted_features[6]
            + predicted_features[7]
        )
        # calculate percentage of special to total sentences
        special_to_sents = total_special / predicted_features[3]

        features = (
            casename,
            predicted_features[0],
            predicted_features[1],
            ents_to_tokens,
            predicted_features[2],
            predicted_features[3],
            predicted_features[4],
            predicted_features[5],
            predicted_features[6],
            predicted_features[7],
            predicted_features[8],
            total_special,
            special_to_sents,
        )
        FEATURES.append(features)
        iter += 1

        if iter > observations:
            break

    df = pd.DataFrame(FEATURES)
    msg.good(f"Exporting sample(s) to {output_file}!")
    df.to_csv(output_file, index=False)


if __name__ == "__main__":
    plac.call(main)
