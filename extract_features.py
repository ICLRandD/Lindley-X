"""
Extract features from plain text judgments for the Lindley-X reportability model.

This script is called by apply_model.py. It receives the contents of the text file as a
string, creates a spaCy Doc and uses the en_blackstone_proto model to assist in the extraction
of the features, which returnes as a list of tuples. 
"""

import pandas as pd
from pathlib import Path
import spacy
from spacy.tokens import Doc
from typing import List, Tuple
from wasabi import Printer
import plac

msg = Printer()
nlp = spacy.load("en_blackstone_proto")

merge_ents = nlp.create_pipe("merge_entities")
nlp.add_pipe(merge_ents, last=True)

FEATURES = []

def predicted_stats(document: str) -> Tuple:
    """
    Extract statistics from the documents. Takes a string 
    and returns a tuple of stats.
    """
    doc = make_doc(document)
    token_count = count_tokens(doc)
    ent_count = count_entities(doc)
    cases_cited_count = total_cases_cited(doc)
    sentence_count = total_sentences(doc)
    cat_counts = total_cat_types(document)
    return (
        token_count,
        ent_count,
        cases_cited_count,
        sentence_count,
        cat_counts[0],
        cat_counts[1],
        cat_counts[2],
        cat_counts[3],
        cat_counts[4],
    )


def make_doc(document: str) -> Doc:
    """
    Creat the spaCy Doc object from the source file.
    Takes a str, returns a Doc.
    """
    # prevent spaCy error: [E088] where string length exceeds 1000000 chars
    if len(document) > 1000000:
        document = document[:999998]
    doc = nlp(document)
    return doc


def count_tokens(doc: Doc) -> int:
    """Count the tokens in the document"""
    total_tokens = len([token for token in doc])
    return total_tokens


def count_entities(doc: Doc) -> int:
    """Count the entities in the document"""
    total_ents = len([ent for ent in doc.ents])
    return total_ents


def entity_to_tokens(total_tokens: int, total_ents: int) -> float:
    """Return the percentage of entities to tokens"""
    return total_ents / total_tokens


def total_cases_cited(doc: Doc) -> int:
    """Total cases cited"""
    total_ents = len([ent for ent in doc.ents if ent.label_ == "CITATION"])
    return total_ents


def total_sentences(doc: Doc) -> int:
    """Total sentences"""
    sentences = len([sent for sent in doc.sents])
    return sentences


def get_top_cat(doc: Doc) -> str:
    """
    Function to identify the highest scoring category
    prediction generated by the text categoriser. 
    """
    cats = doc.cats
    max_score = max(cats.values())
    max_cats = [k for k, v in cats.items() if v == max_score]
    max_cat = max_cats[0]
    return max_cat


def total_cat_types(document: str) -> Tuple:
    """Returns five-value tuple of sentence-level categorisations"""

    doc = make_doc(document)
    sentences = [sent.text for sent in doc.sents]

    uncat = 0
    axiom = 0
    conclusion = 0
    issue = 0
    legal_test = 0

    for doc in nlp.pipe(sentences, batch_size=50):
        top_cat = get_top_cat(doc)
        if top_cat == "AXIOM":
            axiom += 1
        elif top_cat == "CONCLUSION":
            conclusion += 1
        elif top_cat == "ISSUE":
            issue += 1
        elif top_cat == "LEGAL_TEST":
            legal_test += 1
        else:
            uncat += 1

    total_cats = uncat + axiom + conclusion + issue + legal_test
    assert total_cats == len(sentences)

    return (axiom, conclusion, issue, legal_test, uncat)

def feature_extraction(input_dir: Path) -> List:
    directory = Path(input_dir)
    msg.info(f"Iterating over files and extracting sample(s).")
    for filename in directory.iterdir():
        f = open(filename)
        document = f.read()
        document = document.replace("\n", " ")
        casename = filename.stem

        # get predicted features
        predicted_features = predicted_stats(document)
        # calculate percentage of entities to tokens
        if predicted_features[0] != 0 and predicted_features[1] != 0:
            ents_to_tokens = predicted_features[0] / predicted_features[1]
        else:
            break
        # get total special category sentences
        total_special = (
            predicted_features[4]
            + predicted_features[5]
            + predicted_features[6]
            + predicted_features[7]
        )
        # calculate percentage of special to total sentences
        special_to_sents = total_special / predicted_features[3]

        features = (
            casename,
            predicted_features[0],
            predicted_features[1],
            ents_to_tokens,
            predicted_features[2],
            predicted_features[3],
            predicted_features[4],
            predicted_features[5],
            predicted_features[6],
            predicted_features[7],
            predicted_features[8],
            total_special,
            special_to_sents,
        )
        FEATURES.append(features)
    return FEATURES
        
